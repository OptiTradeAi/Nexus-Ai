# üß™ Guia de Testes - Nexus AI

**Vers√£o:** 1.0.0  
**Autor:** Manus AI  
**Data:** 2025-09-29  

## üéØ Objetivo

Este documento fornece um guia abrangente para testar todos os componentes do sistema Nexus AI, desde testes unit√°rios at√© valida√ß√£o em produ√ß√£o. O guia est√° organizado por componente e tipo de teste, com scripts automatizados e procedimentos manuais detalhados.

## üìã Vis√£o Geral dos Testes

### Estrat√©gia de Testes

O Nexus AI utiliza uma estrat√©gia de testes em pir√¢mide, priorizando:

1. **Testes Unit√°rios** (70%): Componentes individuais
2. **Testes de Integra√ß√£o** (20%): Intera√ß√£o entre componentes  
3. **Testes E2E** (10%): Fluxo completo do usu√°rio

### Tipos de Testes

- **Funcionais**: Verificam se o sistema faz o que deveria fazer
- **Performance**: Validam lat√™ncia, throughput e escalabilidade
- **Seguran√ßa**: Testam vulnerabilidades e prote√ß√£o de dados
- **Usabilidade**: Avaliam experi√™ncia do usu√°rio
- **Compatibilidade**: Verificam funcionamento em diferentes ambientes

## üèóÔ∏è Configura√ß√£o do Ambiente de Testes

### Pr√©-requisitos

```bash
# Depend√™ncias de desenvolvimento
pip install pytest pytest-asyncio pytest-cov
pip install selenium webdriver-manager
pip install locust  # Para testes de carga
npm install -g lighthouse  # Para testes de performance web
```

### Estrutura de Testes

```
tests/
‚îú‚îÄ‚îÄ unit/                   # Testes unit√°rios
‚îÇ   ‚îú‚îÄ‚îÄ test_server.py
‚îÇ   ‚îú‚îÄ‚îÄ test_model.py
‚îÇ   ‚îî‚îÄ‚îÄ test_tools.py
‚îú‚îÄ‚îÄ integration/            # Testes de integra√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ test_websocket.py
‚îÇ   ‚îú‚îÄ‚îÄ test_extension.py
‚îÇ   ‚îî‚îÄ‚îÄ test_ocr.py
‚îú‚îÄ‚îÄ e2e/                   # Testes end-to-end
‚îÇ   ‚îú‚îÄ‚îÄ test_full_flow.py
‚îÇ   ‚îî‚îÄ‚îÄ test_user_journey.py
‚îú‚îÄ‚îÄ performance/           # Testes de performance
‚îÇ   ‚îú‚îÄ‚îÄ load_test.py
‚îÇ   ‚îî‚îÄ‚îÄ stress_test.py
‚îú‚îÄ‚îÄ fixtures/              # Dados de teste
‚îÇ   ‚îú‚îÄ‚îÄ sample_ticks.json
‚îÇ   ‚îî‚îÄ‚îÄ sample_candles.json
‚îî‚îÄ‚îÄ conftest.py           # Configura√ß√£o pytest
```

### Configura√ß√£o Base

```python
# conftest.py
import pytest
import asyncio
import json
from pathlib import Path

@pytest.fixture
def sample_ticks():
    """Carrega ticks de exemplo para testes."""
    with open('tests/fixtures/sample_ticks.json') as f:
        return json.load(f)

@pytest.fixture
def test_server():
    """Inicia servidor de teste."""
    # Implementa√ß√£o do servidor de teste
    pass

@pytest.fixture
def browser():
    """Configura navegador para testes E2E."""
    from selenium import webdriver
    driver = webdriver.Chrome()
    yield driver
    driver.quit()
```

## üîß Testes do Servidor (server/)

### Testes Unit√°rios

#### Teste de Agrega√ß√£o de Candles

```python
# tests/unit/test_server.py
import pytest
from server.server import CandleAggregator

class TestCandleAggregator:
    
    def test_candle_creation(self):
        """Testa cria√ß√£o de candle a partir de ticks."""
        aggregator = CandleAggregator('AAPL-OTC', '1m')
        
        # Adicionar ticks
        aggregator.add_tick(100.0, 1234567890)
        aggregator.add_tick(101.0, 1234567891)
        aggregator.add_tick(99.5, 1234567892)
        aggregator.add_tick(100.5, 1234567893)
        
        candle = aggregator.get_current_candle()
        
        assert candle['open'] == 100.0
        assert candle['high'] == 101.0
        assert candle['low'] == 99.5
        assert candle['close'] == 100.5
    
    def test_candle_timeframe(self):
        """Testa agrega√ß√£o por timeframe."""
        aggregator = CandleAggregator('AAPL-OTC', '1m')
        
        # Ticks no mesmo minuto
        base_time = 1234567800  # In√≠cio do minuto
        aggregator.add_tick(100.0, base_time)
        aggregator.add_tick(101.0, base_time + 30)
        
        # Tick no pr√≥ximo minuto
        aggregator.add_tick(102.0, base_time + 60)
        
        # Deve ter criado novo candle
        assert len(aggregator.closed_candles) == 1
        assert aggregator.closed_candles[0]['close'] == 101.0
    
    @pytest.mark.asyncio
    async def test_websocket_broadcast(self):
        """Testa broadcast de candles via WebSocket."""
        from server.server import broadcast_candle
        
        # Mock WebSocket connections
        mock_connections = []
        
        candle_data = {
            'symbol': 'AAPL-OTC',
            'timeframe': '1m',
            'open': 100.0,
            'high': 101.0,
            'low': 99.5,
            'close': 100.5
        }
        
        await broadcast_candle(candle_data, mock_connections)
        
        # Verificar se mensagem foi enviada
        # Implementar verifica√ß√£o baseada no mock
```

#### Teste de Endpoints HTTP

```python
import pytest
from aiohttp.test_utils import AioHTTPTestCase
from server.server import create_app

class TestHTTPEndpoints(AioHTTPTestCase):
    
    async def get_application(self):
        return create_app()
    
    async def test_health_endpoint(self):
        """Testa endpoint de health check."""
        resp = await self.client.request("GET", "/health")
        assert resp.status == 200
        
        data = await resp.json()
        assert data['status'] == 'healthy'
        assert 'uptime' in data
        assert 'version' in data
    
    async def test_push_tick_endpoint(self):
        """Testa endpoint de recebimento de ticks."""
        tick_data = {
            'symbol': 'AAPL-OTC',
            'price': 100.50,
            'ts': 1234567890,
            'volume': 1
        }
        
        resp = await self.client.request(
            "POST", "/push",
            json=tick_data
        )
        
        assert resp.status == 200
        data = await resp.json()
        assert data['status'] == 'received'
    
    async def test_invalid_tick_data(self):
        """Testa valida√ß√£o de dados inv√°lidos."""
        invalid_data = {
            'symbol': 'INVALID',
            'price': 'not_a_number'
        }
        
        resp = await self.client.request(
            "POST", "/push",
            json=invalid_data
        )
        
        assert resp.status == 400
```

### Testes de Performance

#### Teste de Carga

```python
# tests/performance/load_test.py
from locust import HttpUser, task, between
import json
import time
import random

class NexusLoadTest(HttpUser):
    wait_time = between(0.1, 0.5)  # 100-500ms entre requests
    
    def on_start(self):
        """Configura√ß√£o inicial do usu√°rio."""
        self.symbols = ['AAPL-OTC', 'EURUSD-OTC', 'GBPUSD-OTC']
    
    @task(10)
    def send_tick(self):
        """Envia tick para o servidor."""
        symbol = random.choice(self.symbols)
        tick_data = {
            'symbol': symbol,
            'price': round(random.uniform(90, 110), 4),
            'ts': time.time(),
            'volume': random.randint(1, 100),
            'bid_price': round(random.uniform(89, 109), 4),
            'ask_price': round(random.uniform(91, 111), 4)
        }
        
        with self.client.post("/push", json=tick_data, catch_response=True) as response:
            if response.status_code == 200:
                response.success()
            else:
                response.failure(f"Status: {response.status_code}")
    
    @task(2)
    def health_check(self):
        """Verifica health do servidor."""
        with self.client.get("/health", catch_response=True) as response:
            if response.status_code == 200:
                data = response.json()
                if data.get('status') == 'healthy':
                    response.success()
                else:
                    response.failure("Server not healthy")
            else:
                response.failure(f"Status: {response.status_code}")

# Executar teste de carga
# locust -f tests/performance/load_test.py --host=http://localhost:9000
```

#### Teste de Stress

```python
# tests/performance/stress_test.py
import asyncio
import aiohttp
import time
import statistics
from concurrent.futures import ThreadPoolExecutor

async def stress_test_server():
    """Teste de stress do servidor."""
    
    async def send_tick(session, symbol, tick_id):
        """Envia um tick individual."""
        tick_data = {
            'symbol': symbol,
            'price': 100.0 + (tick_id % 100) * 0.01,
            'ts': time.time(),
            'volume': 1
        }
        
        start_time = time.time()
        try:
            async with session.post('http://localhost:9000/push', json=tick_data) as resp:
                await resp.json()
                return time.time() - start_time
        except Exception as e:
            print(f"Erro: {e}")
            return None
    
    # Configura√ß√£o do teste
    concurrent_users = 100
    ticks_per_user = 1000
    symbols = ['AAPL-OTC', 'EURUSD-OTC', 'GBPUSD-OTC']
    
    async with aiohttp.ClientSession() as session:
        tasks = []
        
        # Criar tasks para usu√°rios concorrentes
        for user_id in range(concurrent_users):
            for tick_id in range(ticks_per_user):
                symbol = symbols[tick_id % len(symbols)]
                task = send_tick(session, symbol, tick_id)
                tasks.append(task)
        
        print(f"Iniciando teste de stress: {len(tasks)} requests")
        start_time = time.time()
        
        # Executar todas as tasks
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        end_time = time.time()
        
        # Analisar resultados
        successful_requests = [r for r in results if isinstance(r, float)]
        failed_requests = len(results) - len(successful_requests)
        
        if successful_requests:
            avg_latency = statistics.mean(successful_requests)
            p95_latency = statistics.quantiles(successful_requests, n=20)[18]  # P95
            p99_latency = statistics.quantiles(successful_requests, n=100)[98]  # P99
        else:
            avg_latency = p95_latency = p99_latency = 0
        
        total_time = end_time - start_time
        throughput = len(successful_requests) / total_time
        
        print(f"""
        Resultados do Teste de Stress:
        ==============================
        Total de requests: {len(results)}
        Requests bem-sucedidos: {len(successful_requests)}
        Requests falharam: {failed_requests}
        Taxa de sucesso: {len(successful_requests)/len(results)*100:.2f}%
        
        Lat√™ncia m√©dia: {avg_latency*1000:.2f}ms
        Lat√™ncia P95: {p95_latency*1000:.2f}ms
        Lat√™ncia P99: {p99_latency*1000:.2f}ms
        
        Throughput: {throughput:.2f} requests/segundo
        Tempo total: {total_time:.2f} segundos
        """)

# Executar: python tests/performance/stress_test.py
if __name__ == '__main__':
    asyncio.run(stress_test_server())
```

## üåê Testes da Extens√£o (extension/)

### Teste de Intercepta√ß√£o WebSocket

```python
# tests/integration/test_extension.py
import pytest
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import json
import time

class TestExtension:
    
    @pytest.fixture
    def browser_with_extension(self):
        """Configura navegador com extens√£o carregada."""
        options = webdriver.ChromeOptions()
        options.add_argument(f"--load-extension=./extension")
        options.add_argument("--disable-web-security")
        
        driver = webdriver.Chrome(options=options)
        yield driver
        driver.quit()
    
    def test_extension_loads(self, browser_with_extension):
        """Testa se a extens√£o carrega corretamente."""
        driver = browser_with_extension
        
        # Navegar para p√°gina de teste
        driver.get("chrome://extensions/")
        
        # Verificar se extens√£o est√° listada
        extensions = driver.find_elements(By.CSS_SELECTOR, "[id^='extension-']")
        assert len(extensions) > 0
    
    def test_websocket_interception(self, browser_with_extension):
        """Testa intercepta√ß√£o de WebSocket."""
        driver = browser_with_extension
        
        # Criar p√°gina de teste com WebSocket
        test_html = """
        <!DOCTYPE html>
        <html>
        <head><title>Test WebSocket</title></head>
        <body>
            <script>
                // Simular WebSocket da corretora
                const ws = new WebSocket('wss://ws-us2.pusher.com:443/app/test');
                
                ws.onopen = function() {
                    console.log('WebSocket conectado');
                    
                    // Simular mensagem de tick
                    setTimeout(() => {
                        ws.send(JSON.stringify({
                            event: 'price_update',
                            data: {
                                symbol: 'AAPL-OTC',
                                price: 100.50,
                                timestamp: Date.now()
                            }
                        }));
                    }, 1000);
                };
                
                // Verificar se extens√£o interceptou
                window.addEventListener('nexus:tick', (e) => {
                    window.nexusTickReceived = true;
                    window.nexusTickData = e.detail;
                });
            </script>
        </body>
        </html>
        """
        
        # Salvar e carregar p√°gina de teste
        with open('/tmp/test_websocket.html', 'w') as f:
            f.write(test_html)
        
        driver.get('file:///tmp/test_websocket.html')
        
        # Aguardar intercepta√ß√£o
        WebDriverWait(driver, 10).until(
            lambda d: d.execute_script("return window.nexusTickReceived === true")
        )
        
        # Verificar dados interceptados
        tick_data = driver.execute_script("return window.nexusTickData")
        assert tick_data is not None
        assert tick_data['symbol'] == 'AAPL-OTC'
        assert tick_data['price'] == 100.50
    
    def test_data_forwarding(self, browser_with_extension):
        """Testa envio de dados para servidor."""
        driver = browser_with_extension
        
        # Configurar mock server para receber dados
        import threading
        import http.server
        import socketserver
        
        received_data = []
        
        class MockHandler(http.server.BaseHTTPRequestHandler):
            def do_POST(self):
                if self.path == '/push':
                    content_length = int(self.headers['Content-Length'])
                    post_data = self.rfile.read(content_length)
                    received_data.append(json.loads(post_data))
                    
                    self.send_response(200)
                    self.send_header('Content-Type', 'application/json')
                    self.end_headers()
                    self.wfile.write(b'{"status": "received"}')
        
        # Iniciar mock server
        with socketserver.TCPServer(("", 9001), MockHandler) as httpd:
            server_thread = threading.Thread(target=httpd.serve_forever)
            server_thread.daemon = True
            server_thread.start()
            
            # Simular captura de tick
            driver.execute_script("""
                // Simular evento de tick capturado
                window.dispatchEvent(new CustomEvent('nexus:tick', {
                    detail: {
                        symbol: 'AAPL-OTC',
                        price: 100.50,
                        ts: Date.now() / 1000,
                        volume: 1
                    }
                }));
            """)
            
            # Aguardar recebimento no mock server
            time.sleep(2)
            
            # Verificar se dados foram recebidos
            assert len(received_data) > 0
            assert received_data[0]['symbol'] == 'AAPL-OTC'
            
            httpd.shutdown()
```

### Teste de Compatibilidade

```python
def test_browser_compatibility():
    """Testa compatibilidade com diferentes navegadores."""
    
    browsers = [
        ('chrome', webdriver.Chrome),
        ('edge', webdriver.Edge),
        # ('firefox', webdriver.Firefox)  # Se suportado
    ]
    
    for browser_name, browser_class in browsers:
        try:
            options = browser_class.options() if hasattr(browser_class, 'options') else None
            if options:
                options.add_argument(f"--load-extension=./extension")
            
            driver = browser_class(options=options)
            
            # Teste b√°sico de carregamento
            driver.get("chrome://extensions/")
            time.sleep(2)
            
            # Verificar se extens√£o est√° ativa
            # Implementar verifica√ß√£o espec√≠fica por navegador
            
            driver.quit()
            print(f"‚úÖ {browser_name}: Compat√≠vel")
            
        except Exception as e:
            print(f"‚ùå {browser_name}: Erro - {e}")
```

## üß† Testes do Modelo IA (model/)

### Testes Unit√°rios do Modelo

```python
# tests/unit/test_model.py
import pytest
import torch
import numpy as np
from model.model import NexusTransformer, ModelConfig, FeatureExtractor

class TestNexusModel:
    
    @pytest.fixture
    def model_config(self):
        """Configura√ß√£o de teste para o modelo."""
        return ModelConfig(
            d_model=64,
            nhead=4,
            num_layers=2,
            sequence_length=30,
            feature_dim=12
        )
    
    @pytest.fixture
    def model(self, model_config):
        """Inst√¢ncia do modelo para testes."""
        return NexusTransformer(model_config)
    
    def test_model_forward(self, model, model_config):
        """Testa forward pass do modelo."""
        batch_size = 2
        seq_len = model_config.sequence_length
        feature_dim = model_config.feature_dim
        
        # Criar input de teste
        x = torch.randn(batch_size, seq_len, feature_dim)
        
        # Forward pass
        output = model(x)
        
        # Verificar dimens√µes
        assert output.shape == (batch_size, 2)  # [P(down), P(up)]
        
        # Verificar se output √© v√°lido
        assert not torch.isnan(output).any()
        assert torch.isfinite(output).all()
    
    def test_model_probabilities(self, model, model_config):
        """Testa gera√ß√£o de probabilidades."""
        batch_size = 1
        seq_len = model_config.sequence_length
        feature_dim = model_config.feature_dim
        
        x = torch.randn(batch_size, seq_len, feature_dim)
        
        # Obter probabilidades
        probs = model.predict_probabilities(x)
        
        # Verificar se s√£o probabilidades v√°lidas
        assert probs.shape == (batch_size, 2)
        assert torch.allclose(probs.sum(dim=1), torch.ones(batch_size))
        assert (probs >= 0).all() and (probs <= 1).all()
    
    def test_feature_extractor(self):
        """Testa extrator de features."""
        extractor = FeatureExtractor()
        
        # Dados de teste
        tick_data = [
            {'price': 100.0, 'ts': 1234567890, 'volume': 1, 'bid_price': 99.95, 'ask_price': 100.05},
            {'price': 100.1, 'ts': 1234567891, 'volume': 2, 'bid_price': 100.05, 'ask_price': 100.15},
            {'price': 99.9, 'ts': 1234567892, 'volume': 1, 'bid_price': 99.85, 'ask_price': 99.95},
        ]
        
        # Extrair features
        features = extractor.extract_features(tick_data)
        
        # Verificar dimens√µes
        assert features.shape == (len(tick_data), 12)
        
        # Verificar se features s√£o v√°lidas
        assert not np.isnan(features).any()
        assert np.isfinite(features).all()
    
    def test_model_training_step(self, model, model_config):
        """Testa um passo de treinamento."""
        batch_size = 4
        seq_len = model_config.sequence_length
        feature_dim = model_config.feature_dim
        
        # Dados de entrada
        x = torch.randn(batch_size, seq_len, feature_dim)
        y = torch.randint(0, 2, (batch_size,))  # Labels bin√°rios
        
        # Forward pass
        logits = model(x)
        
        # Calcular loss
        criterion = torch.nn.CrossEntropyLoss()
        loss = criterion(logits, y)
        
        # Verificar se loss √© v√°lido
        assert not torch.isnan(loss)
        assert loss.item() > 0
        
        # Backward pass
        loss.backward()
        
        # Verificar se gradientes foram calculados
        for param in model.parameters():
            if param.requires_grad:
                assert param.grad is not None
```

### Teste de Backtesting

```python
# tests/integration/test_backtesting.py
import pytest
import pandas as pd
import numpy as np
from model.model import create_nexus_model
from model.infer import create_inference_engine

class TestBacktesting:
    
    @pytest.fixture
    def historical_data(self):
        """Dados hist√≥ricos para backtesting."""
        # Gerar dados sint√©ticos para teste
        np.random.seed(42)
        
        dates = pd.date_range('2024-01-01', periods=10000, freq='1min')
        prices = 100 + np.cumsum(np.random.randn(10000) * 0.01)
        
        data = []
        for i, (date, price) in enumerate(zip(dates, prices)):
            data.append({
                'timestamp': date.timestamp(),
                'price': price,
                'volume': np.random.randint(1, 100),
                'bid_price': price - 0.01,
                'ask_price': price + 0.01
            })
        
        return data
    
    def test_backtesting_accuracy(self, historical_data):
        """Testa precis√£o do modelo em dados hist√≥ricos."""
        # Criar modelo
        model = create_nexus_model()
        
        # Simular predi√ß√µes
        correct_predictions = 0
        total_predictions = 0
        
        # Usar janela deslizante para predi√ß√µes
        window_size = 60
        
        for i in range(window_size, len(historical_data) - 20):
            # Sequ√™ncia de entrada
            sequence = historical_data[i-window_size:i]
            
            # Fazer predi√ß√£o
            prediction = model.predict(sequence)
            
            if prediction['signal']:
                total_predictions += 1
                
                # Verificar resultado real ap√≥s 20 per√≠odos
                current_price = historical_data[i]['price']
                future_price = historical_data[i + 20]['price']
                
                actual_direction = 'UP' if future_price > current_price else 'DOWN'
                
                if prediction['signal'] == actual_direction:
                    correct_predictions += 1
        
        # Calcular precis√£o
        if total_predictions > 0:
            accuracy = correct_predictions / total_predictions
            print(f"Precis√£o do backtesting: {accuracy:.2%}")
            print(f"Total de predi√ß√µes: {total_predictions}")
            
            # Verificar se atende crit√©rio m√≠nimo
            assert accuracy >= 0.60  # 60% m√≠nimo para dados sint√©ticos
        else:
            pytest.skip("Nenhuma predi√ß√£o gerada durante o teste")
    
    def test_performance_metrics(self, historical_data):
        """Testa m√©tricas de performance do modelo."""
        engine = create_inference_engine()
        
        # Simular opera√ß√£o por per√≠odo
        start_time = time.time()
        predictions_made = 0
        
        for i, tick in enumerate(historical_data[:1000]):
            result = asyncio.run(engine.process_tick('TEST-SYMBOL', tick))
            if result and result['prediction']['signal']:
                predictions_made += 1
        
        end_time = time.time()
        
        # Calcular m√©tricas
        total_time = end_time - start_time
        throughput = len(historical_data[:1000]) / total_time
        
        print(f"Throughput: {throughput:.2f} ticks/segundo")
        print(f"Predi√ß√µes geradas: {predictions_made}")
        
        # Verificar performance
        assert throughput >= 50  # M√≠nimo 50 ticks/segundo
        
        # Verificar estat√≠sticas do engine
        stats = engine.get_performance_stats()
        assert stats['avg_latency_ms'] < 100  # Lat√™ncia < 100ms
```

## üåê Testes da Interface Web (web/)

### Testes de Interface

```python
# tests/e2e/test_web_interface.py
import pytest
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time

class TestWebInterface:
    
    @pytest.fixture
    def browser(self):
        """Configura navegador para testes."""
        options = webdriver.ChromeOptions()
        options.add_argument("--headless")  # Executar sem interface gr√°fica
        driver = webdriver.Chrome(options=options)
        yield driver
        driver.quit()
    
    def test_chart_loads(self, browser):
        """Testa se o gr√°fico carrega corretamente."""
        driver = browser
        driver.get("http://localhost:8000/chart_comparacao.html")
        
        # Aguardar carregamento do gr√°fico
        chart_container = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.ID, "chart-container"))
        )
        
        assert chart_container is not None
        
        # Verificar se TradingView foi carregado
        tv_chart = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, ".tv-lightweight-charts"))
        )
        
        assert tv_chart is not None
    
    def test_websocket_connection(self, browser):
        """Testa conex√£o WebSocket da interface."""
        driver = browser
        driver.get("http://localhost:8000/chart_comparacao.html")
        
        # Aguardar conex√£o WebSocket
        time.sleep(3)
        
        # Verificar status da conex√£o via JavaScript
        connection_status = driver.execute_script("""
            return window.nexusWebSocket && window.nexusWebSocket.readyState === WebSocket.OPEN;
        """)
        
        assert connection_status is True
    
    def test_candle_rendering(self, browser):
        """Testa renderiza√ß√£o de candles."""
        driver = browser
        driver.get("http://localhost:8000/chart_comparacao.html")
        
        # Simular recebimento de candle via WebSocket
        driver.execute_script("""
            // Simular candle de teste
            const testCandle = {
                type: 'candle:update',
                symbol: 'AAPL-OTC',
                timeframe: '1m',
                start: Date.now() / 1000,
                open: 100.0,
                high: 101.0,
                low: 99.5,
                close: 100.5,
                volume: 1000
            };
            
            // Simular recebimento via WebSocket
            if (window.handleWebSocketMessage) {
                window.handleWebSocketMessage(testCandle);
            }
        """)
        
        # Aguardar renderiza√ß√£o
        time.sleep(2)
        
        # Verificar se candle foi adicionado ao gr√°fico
        candle_count = driver.execute_script("""
            return window.chartData ? window.chartData.length : 0;
        """)
        
        assert candle_count > 0
    
    def test_responsive_design(self, browser):
        """Testa design responsivo."""
        driver = browser
        
        # Testar diferentes resolu√ß√µes
        resolutions = [
            (1920, 1080),  # Desktop
            (1366, 768),   # Laptop
            (768, 1024),   # Tablet
            (375, 667)     # Mobile
        ]
        
        for width, height in resolutions:
            driver.set_window_size(width, height)
            driver.get("http://localhost:8000/chart_comparacao.html")
            
            # Aguardar carregamento
            time.sleep(2)
            
            # Verificar se elementos est√£o vis√≠veis
            chart_container = driver.find_element(By.ID, "chart-container")
            assert chart_container.is_displayed()
            
            # Verificar se gr√°fico se ajustou ao tamanho
            chart_width = driver.execute_script("""
                return document.getElementById('chart-container').offsetWidth;
            """)
            
            # Gr√°fico deve ocupar pelo menos 80% da largura
            assert chart_width >= width * 0.8
```

### Testes de Performance Web

```python
# tests/performance/test_web_performance.py
import pytest
import json
import subprocess
from selenium import webdriver

class TestWebPerformance:
    
    def test_lighthouse_performance(self):
        """Testa performance com Google Lighthouse."""
        
        # Executar Lighthouse
        result = subprocess.run([
            'lighthouse',
            'http://localhost:8000/chart_comparacao.html',
            '--output=json',
            '--quiet',
            '--chrome-flags="--headless"'
        ], capture_output=True, text=True)
        
        if result.returncode == 0:
            lighthouse_data = json.loads(result.stdout)
            
            # Verificar m√©tricas de performance
            performance_score = lighthouse_data['lhr']['categories']['performance']['score']
            
            # M√©tricas espec√≠ficas
            metrics = lighthouse_data['lhr']['audits']
            
            fcp = metrics['first-contentful-paint']['numericValue']  # First Contentful Paint
            lcp = metrics['largest-contentful-paint']['numericValue']  # Largest Contentful Paint
            cls = metrics['cumulative-layout-shift']['numericValue']  # Cumulative Layout Shift
            
            print(f"Performance Score: {performance_score * 100:.1f}/100")
            print(f"First Contentful Paint: {fcp:.0f}ms")
            print(f"Largest Contentful Paint: {lcp:.0f}ms")
            print(f"Cumulative Layout Shift: {cls:.3f}")
            
            # Crit√©rios de aprova√ß√£o
            assert performance_score >= 0.8  # Score m√≠nimo 80/100
            assert fcp <= 2000  # FCP <= 2 segundos
            assert lcp <= 4000  # LCP <= 4 segundos
            assert cls <= 0.1   # CLS <= 0.1
        else:
            pytest.skip("Lighthouse n√£o dispon√≠vel")
    
    def test_memory_usage(self):
        """Testa uso de mem√≥ria da interface."""
        options = webdriver.ChromeOptions()
        options.add_argument("--enable-memory-info")
        
        driver = webdriver.Chrome(options=options)
        
        try:
            driver.get("http://localhost:8000/chart_comparacao.html")
            
            # Aguardar carregamento completo
            time.sleep(5)
            
            # Simular uso intensivo (muitos candles)
            for i in range(1000):
                driver.execute_script(f"""
                    const candle = {{
                        time: {1234567890 + i * 60},
                        open: {100 + i * 0.01},
                        high: {100.5 + i * 0.01},
                        low: {99.5 + i * 0.01},
                        close: {100.2 + i * 0.01}
                    }};
                    
                    if (window.chart && window.chart.addCandle) {{
                        window.chart.addCandle(candle);
                    }}
                """)
                
                if i % 100 == 0:
                    time.sleep(0.1)  # Pequena pausa
            
            # Verificar uso de mem√≥ria
            memory_info = driver.execute_script("""
                return {
                    usedJSHeapSize: performance.memory.usedJSHeapSize,
                    totalJSHeapSize: performance.memory.totalJSHeapSize,
                    jsHeapSizeLimit: performance.memory.jsHeapSizeLimit
                };
            """)
            
            used_mb = memory_info['usedJSHeapSize'] / (1024 * 1024)
            total_mb = memory_info['totalJSHeapSize'] / (1024 * 1024)
            
            print(f"Mem√≥ria usada: {used_mb:.1f}MB")
            print(f"Mem√≥ria total: {total_mb:.1f}MB")
            
            # Verificar se uso de mem√≥ria est√° dentro do limite
            assert used_mb <= 100  # M√°ximo 100MB
            
        finally:
            driver.quit()
```

## üîß Testes de Ferramentas (tools/)

### Teste do Comparador de Candles

```python
# tests/unit/test_tools.py
import pytest
import json
import tempfile
from tools.compare_candles import CandleComparator, CandleData

class TestCandleComparator:
    
    @pytest.fixture
    def sample_candles_a(self):
        """Candles da fonte A."""
        return [
            {
                'type': 'candle:closed',
                'symbol': 'AAPL-OTC',
                'timeframe': '1m',
                'start': 1234567800,
                'open': 100.0,
                'high': 101.0,
                'low': 99.5,
                'close': 100.5,
                'volume': 1000
            },
            {
                'type': 'candle:closed',
                'symbol': 'AAPL-OTC',
                'timeframe': '1m',
                'start': 1234567860,
                'open': 100.5,
                'high': 101.5,
                'low': 100.0,
                'close': 101.0,
                'volume': 1200
            }
        ]
    
    @pytest.fixture
    def sample_candles_b(self):
        """Candles da fonte B (com pequenas diferen√ßas)."""
        return [
            {
                'type': 'candle:closed',
                'symbol': 'AAPL-OTC',
                'timeframe': '1m',
                'start': 1234567800,
                'open': 100.01,  # Pequena diferen√ßa
                'high': 101.0,
                'low': 99.5,
                'close': 100.5,
                'volume': 1000
            },
            {
                'type': 'candle:closed',
                'symbol': 'AAPL-OTC',
                'timeframe': '1m',
                'start': 1234567860,
                'open': 100.5,
                'high': 101.5,
                'low': 100.0,
                'close': 101.0,
                'volume': 1200
            }
        ]
    
    def test_load_candles_from_file(self, sample_candles_a):
        """Testa carregamento de candles de arquivo."""
        comparator = CandleComparator()
        
        # Criar arquivo tempor√°rio
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump(sample_candles_a, f)
            temp_file = f.name
        
        # Carregar candles
        comparator.load_candles_from_file(temp_file, 'source_a')
        
        # Verificar se foram carregados
        assert 'source_a' in comparator.candles_by_source
        assert 'AAPL-OTC' in comparator.candles_by_source['source_a']
        assert '1m' in comparator.candles_by_source['source_a']['AAPL-OTC']
        assert len(comparator.candles_by_source['source_a']['AAPL-OTC']['1m']) == 2
    
    def test_compare_sources(self, sample_candles_a, sample_candles_b):
        """Testa compara√ß√£o entre fontes."""
        comparator = CandleComparator(tolerance_pct=0.1)  # 0.1% toler√¢ncia
        
        # Carregar dados de ambas as fontes
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump(sample_candles_a, f)
            file_a = f.name
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump(sample_candles_b, f)
            file_b = f.name
        
        comparator.load_candles_from_file(file_a, 'source_a')
        comparator.load_candles_from_file(file_b, 'source_b')
        
        # Comparar fontes
        results = comparator.compare_sources('source_a', 'source_b')
        
        # Verificar resultados
        assert len(results) == 2  # Dois candles comparados
        
        # Primeiro candle tem diferen√ßa no open
        first_result = results[0]
        assert first_result.open_diff == 0.01
        assert first_result.open_diff_pct > 0
        assert not first_result.is_significant  # Dentro da toler√¢ncia
        
        # Segundo candle √© id√™ntico
        second_result = results[1]
        assert second_result.open_diff == 0.0
        assert second_result.high_diff == 0.0
        assert second_result.low_diff == 0.0
        assert second_result.close_diff == 0.0
    
    def test_generate_report(self, sample_candles_a, sample_candles_b):
        """Testa gera√ß√£o de relat√≥rio."""
        comparator = CandleComparator()
        
        # Carregar e comparar dados
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump(sample_candles_a, f)
            file_a = f.name
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump(sample_candles_b, f)
            file_b = f.name
        
        comparator.load_candles_from_file(file_a, 'source_a')
        comparator.load_candles_from_file(file_b, 'source_b')
        comparator.compare_sources('source_a', 'source_b')
        
        # Gerar relat√≥rio
        report = comparator.generate_report()
        
        # Verificar estrutura do relat√≥rio
        assert 'summary' in report
        assert 'statistics' in report
        assert 'worst_cases' in report
        assert 'by_symbol' in report
        assert 'by_timeframe' in report
        
        # Verificar dados do summary
        summary = report['summary']
        assert summary['total_comparisons'] == 2
        assert summary['accuracy_rate_pct'] >= 0
```

### Teste do Sanitizador de Logs

```python
def test_log_sanitizer():
    """Testa sanitiza√ß√£o de logs."""
    from tools.sanitize_logs import LogSanitizer
    
    sanitizer = LogSanitizer()
    
    # Texto com dados sens√≠veis
    sensitive_text = """
    {
        "token": "abc123def456",
        "password": "mypassword123",
        "email": "user@example.com",
        "api_key": "sk-1234567890abcdef",
        "phone": "555-123-4567",
        "url": "https://api.example.com/v1/data"
    }
    """
    
    # Sanitizar
    sanitized = sanitizer._sanitize_text(sensitive_text)
    
    # Verificar se dados sens√≠veis foram mascarados
    assert "abc123def456" not in sanitized
    assert "mypassword123" not in sanitized
    assert "user@example.com" not in sanitized
    assert "sk-1234567890abcdef" not in sanitized
    assert "555-123-4567" not in sanitized
    
    # Verificar se estrutura foi preservada
    assert "token" in sanitized
    assert "password" in sanitized
    assert "email" in sanitized
```

## üîÑ Testes End-to-End

### Teste do Fluxo Completo

```python
# tests/e2e/test_full_flow.py
import pytest
import asyncio
import time
import json
from selenium import webdriver
import aiohttp

class TestFullFlow:
    
    @pytest.mark.asyncio
    async def test_complete_trading_flow(self):
        """Testa fluxo completo: captura ‚Üí processamento ‚Üí predi√ß√£o ‚Üí sinal."""
        
        # 1. Iniciar servidor
        # (Assumindo que servidor j√° est√° rodando)
        
        # 2. Configurar navegador com extens√£o
        options = webdriver.ChromeOptions()
        options.add_argument("--load-extension=./extension")
        driver = webdriver.Chrome(options=options)
        
        try:
            # 3. Simular captura de ticks
            async with aiohttp.ClientSession() as session:
                # Enviar sequ√™ncia de ticks
                for i in range(100):
                    tick_data = {
                        'symbol': 'AAPL-OTC',
                        'price': 100.0 + (i % 10) * 0.01,
                        'ts': time.time(),
                        'volume': 1,
                        'bid_price': 99.95 + (i % 10) * 0.01,
                        'ask_price': 100.05 + (i % 10) * 0.01
                    }
                    
                    async with session.post(
                        'http://localhost:9000/push',
                        json=tick_data
                    ) as resp:
                        assert resp.status == 200
                    
                    await asyncio.sleep(0.1)  # 100ms entre ticks
                
                # 4. Verificar se candles foram gerados
                async with session.get('http://localhost:9000/health') as resp:
                    health_data = await resp.json()
                    assert health_data['status'] == 'healthy'
                
                # 5. Conectar WebSocket para receber sinais
                import websockets
                
                signals_received = []
                
                async with websockets.connect('ws://localhost:9000/ws') as websocket:
                    # Aguardar sinais por 30 segundos
                    try:
                        async with asyncio.timeout(30):
                            while True:
                                message = await websocket.recv()
                                data = json.loads(message)
                                
                                if data.get('type') == 'signal':
                                    signals_received.append(data)
                                    print(f"Sinal recebido: {data}")
                                    
                                    # Parar ap√≥s primeiro sinal
                                    break
                    except asyncio.TimeoutError:
                        pass
                
                # 6. Verificar se pelo menos um sinal foi gerado
                # (Pode n√£o gerar sinal se probabilidade < 80%)
                print(f"Sinais recebidos: {len(signals_received)}")
                
                # 7. Verificar interface web
                driver.get("http://localhost:8000/chart_comparacao.html")
                time.sleep(5)  # Aguardar carregamento
                
                # Verificar se gr√°fico est√° funcionando
                chart_loaded = driver.execute_script("""
                    return window.chart !== undefined && window.chartData.length > 0;
                """)
                
                assert chart_loaded, "Gr√°fico n√£o carregou corretamente"
                
        finally:
            driver.quit()
    
    def test_error_recovery(self):
        """Testa recupera√ß√£o de erros."""
        # Simular diferentes cen√°rios de erro
        
        # 1. Servidor indispon√≠vel
        # 2. Dados corrompidos
        # 3. Falha de rede
        # 4. Extens√£o desabilitada
        
        # Implementar testes de recupera√ß√£o
        pass
```

## üìä Relat√≥rios de Teste

### Gera√ß√£o de Relat√≥rios

```python
# tests/generate_report.py
import pytest
import json
import datetime
from pathlib import Path

def generate_test_report():
    """Gera relat√≥rio consolidado de testes."""
    
    # Executar todos os testes com coverage
    result = pytest.main([
        '--cov=server',
        '--cov=model', 
        '--cov=tools',
        '--cov-report=json',
        '--cov-report=html',
        '--json-report',
        '--json-report-file=test_report.json',
        'tests/'
    ])
    
    # Carregar resultados
    with open('test_report.json') as f:
        test_data = json.load(f)
    
    with open('coverage.json') as f:
        coverage_data = json.load(f)
    
    # Gerar relat√≥rio consolidado
    report = {
        'timestamp': datetime.datetime.now().isoformat(),
        'summary': {
            'total_tests': test_data['summary']['total'],
            'passed': test_data['summary']['passed'],
            'failed': test_data['summary']['failed'],
            'skipped': test_data['summary']['skipped'],
            'success_rate': test_data['summary']['passed'] / test_data['summary']['total'] * 100
        },
        'coverage': {
            'total_coverage': coverage_data['totals']['percent_covered'],
            'lines_covered': coverage_data['totals']['covered_lines'],
            'lines_missing': coverage_data['totals']['missing_lines']
        },
        'performance': {
            'total_duration': test_data['duration'],
            'slowest_tests': sorted(
                [(test['nodeid'], test['duration']) for test in test_data['tests']],
                key=lambda x: x[1],
                reverse=True
            )[:10]
        }
    }
    
    # Salvar relat√≥rio
    with open('consolidated_test_report.json', 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"Relat√≥rio gerado: consolidated_test_report.json")
    print(f"Taxa de sucesso: {report['summary']['success_rate']:.1f}%")
    print(f"Cobertura de c√≥digo: {report['coverage']['total_coverage']:.1f}%")

if __name__ == '__main__':
    generate_test_report()
```

## üöÄ Automa√ß√£o de Testes

### CI/CD Pipeline

```yaml
# .github/workflows/test.yml
name: Nexus AI Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      redis:
        image: redis:alpine
        ports:
          - 6379:6379
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install -r server/requirements.txt
        pip install -r tests/requirements.txt
    
    - name: Run unit tests
      run: |
        pytest tests/unit/ -v --cov=server --cov=model
    
    - name: Run integration tests
      run: |
        pytest tests/integration/ -v
    
    - name: Run performance tests
      run: |
        pytest tests/performance/ -v --timeout=300
    
    - name: Generate coverage report
      run: |
        coverage xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
```

### Scripts de Teste Local

```bash
#!/bin/bash
# scripts/run_tests.sh

echo "üß™ Executando testes do Nexus AI..."

# Verificar se servidor est√° rodando
if ! curl -s http://localhost:9000/health > /dev/null; then
    echo "‚ùå Servidor n√£o est√° rodando. Iniciando..."
    cd server && python server.py &
    SERVER_PID=$!
    sleep 5
else
    echo "‚úÖ Servidor j√° est√° rodando"
fi

# Verificar se interface web est√° dispon√≠vel
if ! curl -s http://localhost:8000 > /dev/null; then
    echo "‚ùå Interface web n√£o est√° rodando. Iniciando..."
    cd web && python -m http.server 8000 &
    WEB_PID=$!
    sleep 3
else
    echo "‚úÖ Interface web j√° est√° rodando"
fi

# Executar testes
echo "üîß Executando testes unit√°rios..."
pytest tests/unit/ -v

echo "üîó Executando testes de integra√ß√£o..."
pytest tests/integration/ -v

echo "‚ö° Executando testes de performance..."
pytest tests/performance/ -v --timeout=300

echo "üåê Executando testes E2E..."
pytest tests/e2e/ -v --timeout=600

# Gerar relat√≥rio
echo "üìä Gerando relat√≥rio..."
python tests/generate_report.py

# Cleanup
if [ ! -z "$SERVER_PID" ]; then
    kill $SERVER_PID
fi

if [ ! -z "$WEB_PID" ]; then
    kill $WEB_PID
fi

echo "‚úÖ Testes conclu√≠dos!"
```

## üìã Checklist de Valida√ß√£o

### Antes do Deploy

- [ ] Todos os testes unit√°rios passando (100%)
- [ ] Testes de integra√ß√£o passando (>95%)
- [ ] Testes E2E passando (>90%)
- [ ] Cobertura de c√≥digo >80%
- [ ] Performance dentro dos SLAs
- [ ] Seguran√ßa validada
- [ ] Documenta√ß√£o atualizada

### Crit√©rios de Aceita√ß√£o

- [ ] Lat√™ncia E2E <100ms
- [ ] Precis√£o do modelo >80%
- [ ] Uptime >99.9%
- [ ] Throughput >100 ticks/segundo
- [ ] Interface responsiva
- [ ] Extens√£o compat√≠vel com Chrome/Edge
- [ ] Logs sanitizados automaticamente

---

Este guia de testes fornece uma base s√≥lida para validar todos os aspectos do sistema Nexus AI, garantindo qualidade, performance e confiabilidade em produ√ß√£o.

